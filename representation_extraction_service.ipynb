{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNh5GTeyhbuDLYytohf5yF0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7e1f978770d346a69dc0e43bb1b8615e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "VGG16 - ImageNet",
              "OpenCLIP ViT-B/32"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Select a model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_b58130c4ca6a459a8555379848e40a2e",
            "style": "IPY_MODEL_4ce15d98af664e5a806a6f39f6c78d80"
          }
        },
        "b58130c4ca6a459a8555379848e40a2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ce15d98af664e5a806a6f39f6c78d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a6ae46711f04706b030ef05dfb3ed4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Choose images directory:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_3e12296e1d3c4231846e9692a0b4fa2a",
            "placeholder": "/",
            "style": "IPY_MODEL_181c60a912884267ba692817b9aae4a7",
            "value": "/content/drive/MyDrive/gylab/Semantic influences on perception/Aude Oliva's data/experiment 2 data/images"
          }
        },
        "3e12296e1d3c4231846e9692a0b4fa2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "181c60a912884267ba692817b9aae4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "337bf00e4ffb44ccb1fed3d105bf3b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "BoundedIntTextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "BoundedIntTextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntTextView",
            "continuous_update": false,
            "description": "Batch size:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_f054b1d59d1a405fac3b4ba2897793ee",
            "max": 300,
            "min": 1,
            "step": 1,
            "style": "IPY_MODEL_3d96f45b191547b8969f9cad1f119aa5",
            "value": 1
          }
        },
        "f054b1d59d1a405fac3b4ba2897793ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d96f45b191547b8969f9cad1f119aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5e67113af7a408090944ffb87f8963f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Create RDMs",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_7f2228085d8d47e3b6da7f9a370bae91",
            "style": "IPY_MODEL_10fa861488b24737bfcc008f06ae0d96",
            "tooltip": ""
          }
        },
        "7f2228085d8d47e3b6da7f9a370bae91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10fa861488b24737bfcc008f06ae0d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gylab-TAU/layer_representation_extraction_service/blob/master/representation_extraction_service.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jQzDhxHK2LoM",
        "outputId": "401cf3b0-366c-4db3-c149-f2e24f52e650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'layer_representation_extraction_service'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 97 (delta 51), reused 73 (delta 32), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (97/97), 15.71 KiB | 5.24 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Collecting numpy==1.26.1 (from -r layer_representation_extraction_service/requirements.txt (line 1))\n",
            "  Downloading numpy-1.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting open_clip_torch==2.23.0 (from -r layer_representation_extraction_service/requirements.txt (line 2))\n",
            "  Downloading open_clip_torch-2.23.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Pillow==10.1.0 (from -r layer_representation_extraction_service/requirements.txt (line 3))\n",
            "  Downloading Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from -r layer_representation_extraction_service/requirements.txt (line 5)) (2.1.0+cu118)\n",
            "Collecting torchlens==0.1.12 (from -r layer_representation_extraction_service/requirements.txt (line 6))\n",
            "  Downloading torchlens-0.1.12-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics==1.2.0 (from -r layer_representation_extraction_service/requirements.txt (line 7))\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision==0.16.0 in /usr/local/lib/python3.10/dist-packages (from -r layer_representation_extraction_service/requirements.txt (line 8)) (0.16.0+cu118)\n",
            "Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.10/dist-packages (from -r layer_representation_extraction_service/requirements.txt (line 9)) (4.66.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from open_clip_torch==2.23.0->-r layer_representation_extraction_service/requirements.txt (line 2)) (2023.6.3)\n",
            "Collecting ftfy (from open_clip_torch==2.23.0->-r layer_representation_extraction_service/requirements.txt (line 2))\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub (from open_clip_torch==2.23.0->-r layer_representation_extraction_service/requirements.txt (line 2))\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from open_clip_torch==2.23.0->-r layer_representation_extraction_service/requirements.txt (line 2))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from open_clip_torch==2.23.0->-r layer_representation_extraction_service/requirements.txt (line 2)) (3.20.3)\n",
            "Collecting timm (from open_clip_torch==2.23.0->-r layer_representation_extraction_service/requirements.txt (line 2))\n",
            "  Downloading timm-0.9.8-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r layer_representation_extraction_service/requirements.txt (line 5)) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r layer_representation_extraction_service/requirements.txt (line 5)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r layer_representation_extraction_service/requirements.txt (line 5)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r layer_representation_extraction_service/requirements.txt (line 5)) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r layer_representation_extraction_service/requirements.txt (line 5)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r layer_representation_extraction_service/requirements.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r layer_representation_extraction_service/requirements.txt (line 5)) (2.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (1.5.3)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (7.4.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (7.34.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (0.20.1)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics==1.2.0->-r layer_representation_extraction_service/requirements.txt (line 7))\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0->-r layer_representation_extraction_service/requirements.txt (line 8)) (2.31.0)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics==1.2.0->-r layer_representation_extraction_service/requirements.txt (line 7)) (23.2)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->open_clip_torch==2.23.0->-r layer_representation_extraction_service/requirements.txt (line 2)) (0.2.8)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->open_clip_torch==2.23.0->-r layer_representation_extraction_service/requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6))\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (3.0.39)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (4.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.1.0->-r layer_representation_extraction_service/requirements.txt (line 5)) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (2023.3.post1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (2.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0->-r layer_representation_extraction_service/requirements.txt (line 8)) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0->-r layer_representation_extraction_service/requirements.txt (line 8)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0->-r layer_representation_extraction_service/requirements.txt (line 8)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0->-r layer_representation_extraction_service/requirements.txt (line 8)) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->-r layer_representation_extraction_service/requirements.txt (line 5)) (1.3.0)\n",
            "Collecting safetensors (from timm->open_clip_torch==2.23.0->-r layer_representation_extraction_service/requirements.txt (line 2))\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->torchlens==0.1.12->-r layer_representation_extraction_service/requirements.txt (line 6)) (1.16.0)\n",
            "Installing collected packages: sentencepiece, safetensors, Pillow, numpy, lightning-utilities, jedi, ftfy, huggingface-hub, torchmetrics, torchlens, timm, open_clip_torch\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.26.1 which is incompatible.\n",
            "imageio 2.31.6 requires pillow<10.1.0,>=8.3.2, but you have pillow 10.1.0 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-10.1.0 ftfy-6.1.1 huggingface-hub-0.18.0 jedi-0.19.1 lightning-utilities-0.9.0 numpy-1.26.1 open_clip_torch-2.23.0 safetensors-0.4.0 sentencepiece-0.1.99 timm-0.9.8 torchlens-0.1.12 torchmetrics-1.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!git clone \"https://github.com/gylab-TAU/layer_representation_extraction_service.git\"\n",
        "!pip install -r layer_representation_extraction_service/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7VgVtE852ze",
        "outputId": "66d80abb-322f-4e71-e194-1568c68325bc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from layer_representation_extraction_service.models import *\n",
        "from layer_representation_extraction_service.rdm_calculations import MemoryEfficientRDMCalculator\n",
        "import ipywidgets as widgets\n",
        "from pathlib import Path\n",
        "\n",
        "# create a dropdown widget\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=['VGG16 - ImageNet', 'OpenCLIP ViT-B/32'],\n",
        "    value='VGG16 - ImageNet',\n",
        "    description='Select a model:',\n",
        "    disabled=False,\n",
        ")\n",
        "display(dropdown)\n",
        "\n",
        "# create a text input widget\n",
        "text_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='/',\n",
        "    description='Choose images directory:',\n",
        "    disabled=False\n",
        ")\n",
        "display(text_input)\n",
        "\n",
        "# Input for batch size\n",
        "batch_size_input = widgets.BoundedIntText(\n",
        "    value=1,\n",
        "    min=1,\n",
        "    max=300,\n",
        "    step=1,\n",
        "    description='Batch size:',\n",
        "    disabled=False\n",
        ")\n",
        "display(batch_size_input)\n",
        "\n",
        "\n",
        "# create a start button\n",
        "start_button = widgets.Button(description='Create RDMs')\n",
        "\n",
        "# define a function to be called when the start button is clicked\n",
        "def start_button_clicked(b):\n",
        "  if dropdown.value == 'VGG16 - ImageNet':\n",
        "    model, preprocess, layer_names = get_vgg16_imagenet_resources()\n",
        "  elif dropdown.value == 'OpenCLIP ViT-B/32':\n",
        "    model, preprocess, layer_names = get_clip_vit32_resources()\n",
        "\n",
        "  rdm_calc = MemoryEfficientRDMCalculator(batch_size_input.value)\n",
        "\n",
        "  imgs_dir = Path(text_input.value)\n",
        "  if not imgs_dir.is_dir():\n",
        "    print(\"Path does not lead to a directory. Please must choose a directory \\\n",
        "            containing images and try again.\")\n",
        "  else:\n",
        "    outputs = rdm_calc.calc_rdm(model, preprocess, list(imgs_dir.iterdir()), layer_names)\n",
        "    print(outputs.keys())\n",
        "\n",
        "\n",
        "# attach the function to the start button\n",
        "start_button.on_click(start_button_clicked)\n",
        "\n",
        "display(start_button)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529,
          "referenced_widgets": [
            "7e1f978770d346a69dc0e43bb1b8615e",
            "b58130c4ca6a459a8555379848e40a2e",
            "4ce15d98af664e5a806a6f39f6c78d80",
            "2a6ae46711f04706b030ef05dfb3ed4a",
            "3e12296e1d3c4231846e9692a0b4fa2a",
            "181c60a912884267ba692817b9aae4a7",
            "337bf00e4ffb44ccb1fed3d105bf3b0e",
            "f054b1d59d1a405fac3b4ba2897793ee",
            "3d96f45b191547b8969f9cad1f119aa5",
            "d5e67113af7a408090944ffb87f8963f",
            "7f2228085d8d47e3b6da7f9a370bae91",
            "10fa861488b24737bfcc008f06ae0d96"
          ]
        },
        "id": "_XBN-Q9d4g7j",
        "outputId": "30ae1729-266e-43cc-93fd-2f2e578a82df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
            "  torch.has_cuda,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
            "  torch.has_cudnn,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
            "  torch.has_mps,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
            "  torch.has_mkldnn,\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Select a model:', options=('VGG16 - ImageNet', 'OpenCLIP ViT-B/32'), value='VGG16 - Imag…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7e1f978770d346a69dc0e43bb1b8615e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Choose images directory:', placeholder='/')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a6ae46711f04706b030ef05dfb3ed4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "BoundedIntText(value=1, description='Batch size:', max=300, min=1)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "337bf00e4ffb44ccb1fed3d105bf3b0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Create RDMs', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5e67113af7a408090944ffb87f8963f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:11<00:00, 46.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_batches = 118 / 1 = 118.0\n",
            "n_iters = 118.0 ^ 2 = 13924.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/118 [00:00<?, ?it/s]\n",
            "  0%|          | 0/118 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|          | 1/118 [00:10<19:36, 10.05s/it]\u001b[A\n",
            "  2%|▏         | 2/118 [00:18<18:09,  9.39s/it]\u001b[A\n",
            "  3%|▎         | 3/118 [00:29<18:54,  9.87s/it]\u001b[A\n",
            "  3%|▎         | 4/118 [00:37<17:42,  9.32s/it]\u001b[A\n",
            "  4%|▍         | 5/118 [00:47<17:27,  9.27s/it]\u001b[A\n",
            "  5%|▌         | 6/118 [00:56<17:19,  9.28s/it]\u001b[A\n",
            "  6%|▌         | 7/118 [01:04<16:25,  8.88s/it]\u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from layer_representation_extraction_service.models import *\n",
        "from layer_representation_extraction_service.rdm_calculations import MemoryEfficientRDMCalculator\n",
        "import ipywidgets as widgets\n",
        "from pathlib import Path\n",
        "\n",
        "model, preprocess, layer_names = get_vgg16_imagenet_resources()\n",
        "rdm_calc = MemoryEfficientRDMCalculator(118)\n",
        "imgs_dir = Path(\"/content/drive/MyDrive/gylab/Semantic influences on perception/Aude Oliva's data/experiment 2 data/images\")\n",
        "outputs = rdm_calc.calc_rdm(model, preprocess, list(imgs_dir.iterdir()), layer_names)\n",
        "print(outputs.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "uO4XfmII4hDa",
        "outputId": "0ae56508-d674-47f4-f8ef-5ab848af555a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
            "  torch.has_cuda,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
            "  torch.has_cudnn,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
            "  torch.has_mps,\n",
            "/usr/local/lib/python3.10/dist-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
            "  torch.has_mkldnn,\n",
            "  0%|          | 0/1 [01:12<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-50251e4660aa>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrdm_calc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMemoryEfficientRDMCalculator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m118\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimgs_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/gylab/Semantic influences on perception/Aude Oliva's data/experiment 2 data/images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdm_calc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalc_rdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/layer_representation_extraction_service/rdm_calculations/memory_efficient.py\u001b[0m in \u001b[0;36mcalc_rdm\u001b[0;34m(self, model, preprocess, imgs_paths, layers_names)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mrdm_rows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_rdm_row\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'input'"
          ]
        }
      ]
    }
  ]
}